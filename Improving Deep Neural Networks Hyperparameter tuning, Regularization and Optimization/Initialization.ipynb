{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Welcome to the first assignment of \"Improving Deep Neural Networks\". \n",
    "\n",
    "Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n",
    "\n",
    "If you completed the previous course of this specialization, you probably followed our instructions for weight initialization, and it has worked out so far. But how do you choose the initialization for a new neural network? In this notebook, you will see how different initializations lead to different results. \n",
    "\n",
    "A well chosen initialization can:\n",
    "- Speed up the convergence of gradient descent\n",
    "- Increase the odds of gradient descent converging to a lower training (and generalization) error \n",
    "\n",
    "To get started, run the following cell to load the packages and the planar dataset you will try to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 40\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Neural Network model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a 3-layer neural network (already implemented for you). Here are the initialization methods you will experiment with:  \n",
    "- *Zeros initialization* --  setting `initialization = \"zeros\"` in the input argument.\n",
    "- *Random initialization* -- setting `initialization = \"random\"` in the input argument. This initializes the weights to large random values.  \n",
    "- *He initialization* -- setting `initialization = \"he\"` in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015. \n",
    "\n",
    "**Instructions**: Please quickly read over the code below, and run it. In the next part you will implement the three initialization methods that this `model()` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13)\n",
      "(152, 13)\n",
      "(354,)\n",
      "(152,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "bos = load_boston()\n",
    "bos.keys()\n",
    "\n",
    "df = pd.DataFrame(bos.data)\n",
    "df.columns = bos.feature_names\n",
    "df['Price'] = bos.target\n",
    "df.head()\n",
    "\n",
    "data = df[df.columns[:-1]]\n",
    "data = data.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "data['Price'] = df.Price\n",
    "X = data.drop('Price', axis=1).to_numpy()\n",
    "Y = data['Price'].to_numpy()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float).view(-1, 1)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# datasets = TensorDataset(X_train, Y_train)\n",
    "# train_set = DataLoader(datasets, batch_size=10, shuffle=True)\n",
    "\n",
    "# datasets = TensorDataset(X_test, Y_test)\n",
    "# test_set = DataLoader(datasets, batch_size=10, shuffle=True)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(X_train), Variable(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, X_train, Y_train, X_test, Y_test, batch_size, patience=5000, learning_rate = 0.1, best_loss = 1e06):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    iter = 0\n",
    "    while(best_loss>1e-06):    \n",
    "        for i in range(len(X_train)//batch_size):\n",
    "            inputs = Variable(X_train)\n",
    "            labels = Variable(Y_train)\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            out = net(inputs)\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "        # Calculate Accuracy         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Iterate through test dataset\n",
    "        for j in range(len(X_test)//batch_size):\n",
    "            inputs = Variable(X_test)\n",
    "            labels = Variable(Y_test)\n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            val_loss = criterion(outputs, labels)\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "\n",
    "           # Total correct predictions\n",
    "            correct += (outputs.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "        accuracy = 100. * correct.item() / total\n",
    "\n",
    "        # Print Loss\n",
    "        if best_loss > val_loss.item():\n",
    "            p = patience\n",
    "            best_loss = val_loss.item()\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, val_loss.item(), accuracy))\n",
    "        else:\n",
    "            p -= 1\n",
    "            if p == 0:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Zero initialization\n",
    "\n",
    "There are two types of parameters to initialize in a neural network:\n",
    "- the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "\n",
    "**Exercise**: Implement the following function to initialize all parameters to zeros. https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.constant_\n",
    "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.zeros_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train your model on 15,000 iterations using zeros initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w_num = X_train.shape[1]\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(w_num, 1)\n",
    ")\n",
    "\n",
    "nn.init.constant_(net[0].weight, val=0)\n",
    "nn.init.constant_(net[0].bias, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 422.0935974121094. Accuracy: 0.0\n",
      "Iteration: 2. Loss: 331.7978515625. Accuracy: 0.0\n",
      "Iteration: 3. Loss: 252.31092834472656. Accuracy: 0.0\n",
      "Iteration: 4. Loss: 186.39996337890625. Accuracy: 0.0\n",
      "Iteration: 5. Loss: 136.51019287109375. Accuracy: 0.0\n",
      "Iteration: 6. Loss: 99.80962371826172. Accuracy: 0.0\n",
      "Iteration: 7. Loss: 73.47848510742188. Accuracy: 0.0\n",
      "Iteration: 8. Loss: 55.1191291809082. Accuracy: 0.0\n",
      "Iteration: 9. Loss: 42.69768524169922. Accuracy: 0.0\n",
      "Iteration: 10. Loss: 34.540130615234375. Accuracy: 0.0\n",
      "Iteration: 11. Loss: 29.339153289794922. Accuracy: 0.0\n",
      "Iteration: 12. Loss: 26.117141723632812. Accuracy: 0.0\n",
      "Iteration: 13. Loss: 24.174522399902344. Accuracy: 0.0\n",
      "Iteration: 14. Loss: 23.03174591064453. Accuracy: 0.0\n",
      "Iteration: 15. Loss: 22.373502731323242. Accuracy: 0.0\n",
      "Iteration: 16. Loss: 22.000469207763672. Accuracy: 0.0\n",
      "Iteration: 17. Loss: 21.79120635986328. Accuracy: 0.0\n",
      "Iteration: 18. Loss: 21.674148559570312. Accuracy: 0.0\n",
      "Iteration: 19. Loss: 21.608360290527344. Accuracy: 0.0\n",
      "Iteration: 20. Loss: 21.570945739746094. Accuracy: 0.0\n",
      "Iteration: 21. Loss: 21.54930877685547. Accuracy: 0.0\n",
      "Iteration: 22. Loss: 21.53658103942871. Accuracy: 0.0\n",
      "Iteration: 23. Loss: 21.528968811035156. Accuracy: 0.0\n",
      "Iteration: 24. Loss: 21.524368286132812. Accuracy: 0.0\n",
      "Iteration: 25. Loss: 21.521577835083008. Accuracy: 0.0\n",
      "Iteration: 26. Loss: 21.51988410949707. Accuracy: 0.0\n",
      "Iteration: 27. Loss: 21.51886749267578. Accuracy: 0.0\n",
      "Iteration: 28. Loss: 21.518260955810547. Accuracy: 0.0\n",
      "Iteration: 29. Loss: 21.517908096313477. Accuracy: 0.0\n",
      "Iteration: 30. Loss: 21.517698287963867. Accuracy: 0.0\n",
      "Iteration: 31. Loss: 21.51758575439453. Accuracy: 0.0\n",
      "Iteration: 32. Loss: 21.51751708984375. Accuracy: 0.0\n",
      "Iteration: 33. Loss: 21.51748275756836. Accuracy: 0.0\n",
      "Iteration: 34. Loss: 21.5174617767334. Accuracy: 0.0\n",
      "Iteration: 35. Loss: 21.517459869384766. Accuracy: 0.0\n",
      "Iteration: 40. Loss: 21.517457962036133. Accuracy: 0.0\n",
      "Iteration: 55. Loss: 21.5174560546875. Accuracy: 0.0\n",
      "Iteration: 58. Loss: 21.517454147338867. Accuracy: 0.0\n",
      "Iteration: 62. Loss: 21.517452239990234. Accuracy: 0.0\n",
      "Iteration: 76. Loss: 21.5174503326416. Accuracy: 0.0\n",
      "Iteration: 110. Loss: 21.51744842529297. Accuracy: 0.0\n",
      "Iteration: 124. Loss: 21.517446517944336. Accuracy: 0.0\n",
      "Iteration: 136. Loss: 21.517444610595703. Accuracy: 0.0\n",
      "Iteration: 137. Loss: 21.51744270324707. Accuracy: 0.0\n",
      "Iteration: 198. Loss: 21.517440795898438. Accuracy: 0.0\n",
      "Iteration: 200. Loss: 21.51702880859375. Accuracy: 0.0\n",
      "Iteration: 201. Loss: 21.464902877807617. Accuracy: 0.0\n",
      "Iteration: 211. Loss: 21.39563751220703. Accuracy: 0.0\n",
      "Iteration: 231. Loss: 21.39488983154297. Accuracy: 0.0\n",
      "Iteration: 756. Loss: 21.370689392089844. Accuracy: 0.0\n",
      "Iteration: 1022. Loss: 21.3678035736084. Accuracy: 0.0\n",
      "Iteration: 1462. Loss: 21.3602237701416. Accuracy: 0.0\n",
      "Iteration: 4101. Loss: 21.356830596923828. Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "training(net, X_train, Y_train, X_test, Y_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Random initialization\n",
    "\n",
    "To break symmetry, lets intialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you will see what happens if the weights are intialized randomly, but to very large values. \n",
    "\n",
    "**Exercise**: Implement the following function to initialize your weights to large random values (scaled by \\*10) and your biases to zeros. You can choose which type of random distributions.\n",
    "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.uniform_\n",
    "https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w_num = X_train.shape[1]\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(w_num, 1)\n",
    ")\n",
    "\n",
    "nn.init.normal_(net[0].weight, mean=0, std=0.1)\n",
    "nn.init.constant_(net[0].bias, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 422.0658874511719. Accuracy: 0.0\n",
      "Iteration: 2. Loss: 331.5777282714844. Accuracy: 0.0\n",
      "Iteration: 3. Loss: 252.1563262939453. Accuracy: 0.0\n",
      "Iteration: 4. Loss: 186.18951416015625. Accuracy: 0.0\n",
      "Iteration: 5. Loss: 136.3537139892578. Accuracy: 0.0\n",
      "Iteration: 6. Loss: 99.70662689208984. Accuracy: 0.0\n",
      "Iteration: 7. Loss: 73.39952087402344. Accuracy: 0.0\n",
      "Iteration: 8. Loss: 55.06049728393555. Accuracy: 0.0\n",
      "Iteration: 9. Loss: 42.65496063232422. Accuracy: 0.0\n",
      "Iteration: 10. Loss: 34.5093879699707. Accuracy: 0.0\n",
      "Iteration: 11. Loss: 29.317346572875977. Accuracy: 0.0\n",
      "Iteration: 12. Loss: 26.101869583129883. Accuracy: 0.0\n",
      "Iteration: 13. Loss: 24.163951873779297. Accuracy: 0.0\n",
      "Iteration: 14. Loss: 23.024532318115234. Accuracy: 0.0\n",
      "Iteration: 15. Loss: 22.368633270263672. Accuracy: 0.0\n",
      "Iteration: 16. Loss: 21.997209548950195. Accuracy: 0.0\n",
      "Iteration: 17. Loss: 21.789033889770508. Accuracy: 0.0\n",
      "Iteration: 18. Loss: 21.672712326049805. Accuracy: 0.0\n",
      "Iteration: 19. Loss: 21.607419967651367. Accuracy: 0.0\n",
      "Iteration: 20. Loss: 21.570329666137695. Accuracy: 0.0\n",
      "Iteration: 21. Loss: 21.548921585083008. Accuracy: 0.0\n",
      "Iteration: 22. Loss: 21.536331176757812. Accuracy: 0.0\n",
      "Iteration: 23. Loss: 21.528810501098633. Accuracy: 0.0\n",
      "Iteration: 24. Loss: 21.524276733398438. Accuracy: 0.0\n",
      "Iteration: 25. Loss: 21.52151870727539. Accuracy: 0.0\n",
      "Iteration: 26. Loss: 21.519855499267578. Accuracy: 0.0\n",
      "Iteration: 27. Loss: 21.51884651184082. Accuracy: 0.0\n",
      "Iteration: 28. Loss: 21.518245697021484. Accuracy: 0.0\n",
      "Iteration: 29. Loss: 21.517898559570312. Accuracy: 0.0\n",
      "Iteration: 30. Loss: 21.5176944732666. Accuracy: 0.0\n",
      "Iteration: 31. Loss: 21.517578125. Accuracy: 0.0\n",
      "Iteration: 32. Loss: 21.517518997192383. Accuracy: 0.0\n",
      "Iteration: 33. Loss: 21.517478942871094. Accuracy: 0.0\n",
      "Iteration: 34. Loss: 21.51746368408203. Accuracy: 0.0\n",
      "Iteration: 35. Loss: 21.517459869384766. Accuracy: 0.0\n",
      "Iteration: 36. Loss: 21.517457962036133. Accuracy: 0.0\n",
      "Iteration: 38. Loss: 21.5174560546875. Accuracy: 0.0\n",
      "Iteration: 56. Loss: 21.517454147338867. Accuracy: 0.0\n",
      "Iteration: 61. Loss: 21.517452239990234. Accuracy: 0.0\n",
      "Iteration: 79. Loss: 21.5174503326416. Accuracy: 0.0\n",
      "Iteration: 106. Loss: 21.51744842529297. Accuracy: 0.0\n",
      "Iteration: 109. Loss: 21.517446517944336. Accuracy: 0.0\n",
      "Iteration: 122. Loss: 21.517444610595703. Accuracy: 0.0\n",
      "Iteration: 139. Loss: 21.51744270324707. Accuracy: 0.0\n",
      "Iteration: 199. Loss: 21.51287841796875. Accuracy: 0.0\n",
      "Iteration: 216. Loss: 21.489089965820312. Accuracy: 0.0\n",
      "Iteration: 237. Loss: 21.4804744720459. Accuracy: 0.0\n",
      "Iteration: 305. Loss: 21.442903518676758. Accuracy: 0.0\n",
      "Iteration: 336. Loss: 21.404712677001953. Accuracy: 0.0\n",
      "Iteration: 814. Loss: 21.38812828063965. Accuracy: 0.0\n",
      "Iteration: 1484. Loss: 21.3671932220459. Accuracy: 0.0\n",
      "Iteration: 2622. Loss: 21.356639862060547. Accuracy: 0.0\n",
      "Iteration: 2938. Loss: 21.354843139648438. Accuracy: 0.0\n",
      "Iteration: 5391. Loss: 21.352184295654297. Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "training(net, X_train, Y_train, X_test, Y_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - He initialization\n",
    "\n",
    "Finally, try \"He Initialization\"; this is named for the first author of He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])` where He initialization would use `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "**Exercise**: Implement the following function to initialize your parameters with He initialization.\n",
    "\n",
    "**Hint**: This function is similar to the previous `initialize_parameters_random(...)`. The only difference is that instead of multiplying `np.random.randn(..,..)` by 10, you will multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w_num = X_train.shape[1]\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(w_num, 1)\n",
    ")\n",
    "\n",
    "nn.init.xavier_normal_(net[0].weight, gain=1.0)\n",
    "nn.init.constant_(net[0].bias, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 424.1647033691406. Accuracy: 0.0\n",
      "Iteration: 2. Loss: 332.1796875. Accuracy: 0.0\n",
      "Iteration: 3. Loss: 251.9687042236328. Accuracy: 0.0\n",
      "Iteration: 4. Loss: 185.84878540039062. Accuracy: 0.0\n",
      "Iteration: 5. Loss: 136.1215362548828. Accuracy: 0.0\n",
      "Iteration: 6. Loss: 99.51551818847656. Accuracy: 0.0\n",
      "Iteration: 7. Loss: 73.26202392578125. Accuracy: 0.0\n",
      "Iteration: 8. Loss: 54.966861724853516. Accuracy: 0.0\n",
      "Iteration: 9. Loss: 42.59405517578125. Accuracy: 0.0\n",
      "Iteration: 10. Loss: 34.4719352722168. Accuracy: 0.0\n",
      "Iteration: 11. Loss: 29.295969009399414. Accuracy: 0.0\n",
      "Iteration: 12. Loss: 26.090869903564453. Accuracy: 0.0\n",
      "Iteration: 13. Loss: 24.159215927124023. Accuracy: 0.0\n",
      "Iteration: 14. Loss: 23.02326202392578. Accuracy: 0.0\n",
      "Iteration: 15. Loss: 22.369064331054688. Accuracy: 0.0\n",
      "Iteration: 16. Loss: 21.99831199645996. Accuracy: 0.0\n",
      "Iteration: 17. Loss: 21.790246963500977. Accuracy: 0.0\n",
      "Iteration: 18. Loss: 21.673789978027344. Accuracy: 0.0\n",
      "Iteration: 19. Loss: 21.608253479003906. Accuracy: 0.0\n",
      "Iteration: 20. Loss: 21.570940017700195. Accuracy: 0.0\n",
      "Iteration: 21. Loss: 21.54933738708496. Accuracy: 0.0\n",
      "Iteration: 22. Loss: 21.536602020263672. Accuracy: 0.0\n",
      "Iteration: 23. Loss: 21.528987884521484. Accuracy: 0.0\n",
      "Iteration: 24. Loss: 21.524381637573242. Accuracy: 0.0\n",
      "Iteration: 25. Loss: 21.52158546447754. Accuracy: 0.0\n",
      "Iteration: 26. Loss: 21.519886016845703. Accuracy: 0.0\n",
      "Iteration: 27. Loss: 21.518869400024414. Accuracy: 0.0\n",
      "Iteration: 28. Loss: 21.518260955810547. Accuracy: 0.0\n",
      "Iteration: 29. Loss: 21.517902374267578. Accuracy: 0.0\n",
      "Iteration: 30. Loss: 21.517698287963867. Accuracy: 0.0\n",
      "Iteration: 31. Loss: 21.5175838470459. Accuracy: 0.0\n",
      "Iteration: 32. Loss: 21.51752281188965. Accuracy: 0.0\n",
      "Iteration: 33. Loss: 21.517484664916992. Accuracy: 0.0\n",
      "Iteration: 34. Loss: 21.5174617767334. Accuracy: 0.0\n",
      "Iteration: 36. Loss: 21.517457962036133. Accuracy: 0.0\n",
      "Iteration: 56. Loss: 21.5174560546875. Accuracy: 0.0\n",
      "Iteration: 58. Loss: 21.517454147338867. Accuracy: 0.0\n",
      "Iteration: 62. Loss: 21.517452239990234. Accuracy: 0.0\n",
      "Iteration: 90. Loss: 21.5174503326416. Accuracy: 0.0\n",
      "Iteration: 105. Loss: 21.517446517944336. Accuracy: 0.0\n",
      "Iteration: 106. Loss: 21.517444610595703. Accuracy: 0.0\n",
      "Iteration: 136. Loss: 21.51744270324707. Accuracy: 0.0\n",
      "Iteration: 199. Loss: 21.517438888549805. Accuracy: 0.0\n",
      "Iteration: 202. Loss: 21.516569137573242. Accuracy: 0.0\n",
      "Iteration: 203. Loss: 21.491273880004883. Accuracy: 0.0\n",
      "Iteration: 213. Loss: 21.40636444091797. Accuracy: 0.0\n",
      "Iteration: 420. Loss: 21.39154815673828. Accuracy: 0.0\n",
      "Iteration: 598. Loss: 21.38630485534668. Accuracy: 0.0\n",
      "Iteration: 812. Loss: 21.371023178100586. Accuracy: 0.0\n",
      "Iteration: 997. Loss: 21.360458374023438. Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "training(net, X_train, Y_train, X_test, Y_test, batch_size)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "XOESP",
   "launcher_item_id": "8IhFN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
